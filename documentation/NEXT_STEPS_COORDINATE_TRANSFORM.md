# üéØ IMMEDIATE NEXT STEPS
## What to Do Right Now (Post-Perception)

---

## üèÜ **YOU'RE CRUSHING IT!**

**Model Performance:**
- ‚úÖ **90.4% Precision** (when it says "cone", it's correct)
- ‚úÖ **74.7% mAP50** (industry standard metric)
- ‚úÖ **4.5ms inference** (real-time capable)
- ‚úÖ **Perfect color classification** (blue left, yellow right)

**This is COMPETITION-GRADE detection!** üî•

---

## üìã **Next 2 Weeks: Coordinate Transformation**

### **Goal:** Convert YOLO bounding boxes ‚Üí real-world positions in meters

### **Why This Matters:**
Your YOLO outputs: `[x1, y1, x2, y2, confidence, class]` (pixels)
Pipeline needs: `[(x, y, color)]` (meters from car)

---

## üõ†Ô∏è **Step-by-Step Implementation**

### **Step 1: Extract Camera Parameters (Day 1)**

**File to study:**
```
REFERENCES/eufs_sim/eufs_description/sensors/zed.urdf.xacro
```

**What to extract:**
- Image width/height (e.g., 1280x720)
- Horizontal FOV (field of view)
- Camera mounting height above ground
- Camera tilt angle

**Create this file:**
```python
# MIDDLESEX-UNI/perception/camera_params.py

class CameraParams:
    """Camera calibration parameters"""
    
    # Image dimensions
    IMAGE_WIDTH = 1280  # pixels
    IMAGE_HEIGHT = 720  # pixels
    
    # Camera intrinsics (from EUFS sim)
    FOCAL_LENGTH_X = 600  # pixels (REPLACE with actual value)
    FOCAL_LENGTH_Y = 600  # pixels (REPLACE with actual value)
    PRINCIPAL_POINT_X = 640  # pixels (image center)
    PRINCIPAL_POINT_Y = 360  # pixels (image center)
    
    # Camera extrinsics (mounting position)
    CAMERA_HEIGHT = 0.5  # meters above ground (REPLACE)
    CAMERA_TILT = 0.0  # radians (REPLACE)
    
    # Cone physical properties
    CONE_HEIGHT = 0.325  # meters (FS regulation)
```

---

### **Step 2: Implement Distance Estimation (Day 2-3)**

**Create this file:**
```python
# MIDDLESEX-UNI/perception/coordinate_transform.py

import numpy as np
from camera_params import CameraParams

def bbox_to_distance(bbox, camera_params):
    """
    Estimate distance to cone using similar triangles
    
    Args:
        bbox: [x1, y1, x2, y2] in pixels
        camera_params: CameraParams instance
    
    Returns:
        distance in meters
    """
    # 1. Calculate cone height in pixels
    cone_height_pixels = bbox[3] - bbox[1]
    
    # 2. Use similar triangles
    # Real height / Distance = Pixel height / Focal length
    # Distance = (Real height * Focal length) / Pixel height
    
    distance = (camera_params.CONE_HEIGHT * camera_params.FOCAL_LENGTH_Y) / cone_height_pixels
    
    return distance

def bbox_to_lateral_offset(bbox, distance, camera_params):
    """
    Calculate lateral offset (left/right) from camera center
    
    Args:
        bbox: [x1, y1, x2, y2] in pixels
        distance: distance to cone in meters
        camera_params: CameraParams instance
    
    Returns:
        lateral_offset in meters (negative = left, positive = right)
    """
    # 1. Find cone center in image
    cone_center_x = (bbox[0] + bbox[2]) / 2
    
    # 2. Calculate pixel offset from image center
    pixel_offset = cone_center_x - camera_params.PRINCIPAL_POINT_X
    
    # 3. Convert to meters using perspective projection
    lateral_offset = (pixel_offset * distance) / camera_params.FOCAL_LENGTH_X
    
    return lateral_offset

def yolo_to_world_coordinates(detections, camera_params):
    """
    Convert YOLO detections to world coordinates
    
    Args:
        detections: List of YOLO detections
                   Each: {'bbox': [x1,y1,x2,y2], 'confidence': float, 'class': str}
        camera_params: CameraParams instance
    
    Returns:
        List of cones in world coordinates
        Each: {'x': float, 'y': float, 'color': str, 'confidence': float}
    """
    world_cones = []
    
    for det in detections:
        bbox = det['bbox']
        
        # Calculate distance (forward direction)
        distance = bbox_to_distance(bbox, camera_params)
        
        # Calculate lateral offset (left/right)
        lateral = bbox_to_lateral_offset(bbox, distance, camera_params)
        
        # In car frame: x = forward, y = left
        cone = {
            'x': distance,
            'y': lateral,
            'color': det['class'][0],  # 'blue_cone' -> 'b'
            'confidence': det['confidence']
        }
        
        world_cones.append(cone)
    
    return world_cones
```

---

### **Step 3: Test & Validate (Day 4-5)**

**Create test script:**
```python
# MIDDLESEX-UNI/perception/test_coordinate_transform.py

import cv2
import numpy as np
from ultralytics import YOLO
from coordinate_transform import yolo_to_world_coordinates
from camera_params import CameraParams

def test_on_image(image_path, model_path):
    """Test coordinate transformation on a single image"""
    
    # Load model
    model = YOLO(model_path)
    
    # Load image
    img = cv2.imread(image_path)
    
    # Run detection
    results = model(img)[0]
    
    # Convert to our format
    detections = []
    for box in results.boxes:
        det = {
            'bbox': box.xyxy[0].cpu().numpy(),
            'confidence': float(box.conf),
            'class': results.names[int(box.cls)]
        }
        detections.append(det)
    
    # Transform to world coordinates
    camera_params = CameraParams()
    world_cones = yolo_to_world_coordinates(detections, camera_params)
    
    # Print results
    print(f"\nüéØ Detected {len(world_cones)} cones:")
    for i, cone in enumerate(world_cones):
        print(f"  Cone {i+1}: {cone['x']:.2f}m ahead, {cone['y']:.2f}m {'left' if cone['y'] < 0 else 'right'}, color={cone['color']}")
    
    # Visualize
    visualize_detections(img, world_cones, results.boxes)
    
    return world_cones

def visualize_detections(img, world_cones, boxes):
    """Draw detections with world coordinates"""
    
    for cone, box in zip(world_cones, boxes):
        # Draw bbox
        x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())
        color = (255, 0, 0) if cone['color'] == 'b' else (0, 255, 255)
        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
        
        # Draw world coordinates
        label = f"{cone['x']:.1f}m, {cone['y']:.1f}m"
        cv2.putText(img, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
    
    cv2.imshow("Detections with World Coordinates", img)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

if __name__ == "__main__":
    test_on_image(
        "path/to/test/image.jpg",
        "dataset_prep/runs/train/cone_detector_s/weights/best.pt"
    )
```

---

### **Step 4: Validate Accuracy (Day 6-7)**

**How to validate:**

1. **Use EUFS Sim Ground Truth**
   - Run sim, take screenshot
   - Get actual cone positions from sim
   - Compare to your detections
   - Calculate error

2. **Manual Measurement**
   - Use known track layouts
   - Measure cone spacing (typically 3-5m)
   - Verify your estimates match

3. **Sanity Checks**
   - Cones should be 0-20m away
   - Lateral offset should be ¬±5m max
   - Blue cones on left (negative y)
   - Yellow cones on right (positive y)

**Target Accuracy:**
- Distance error: <0.5m
- Lateral error: <0.3m

---

## üìä **Expected Output**

**Before (YOLO):**
```
Detection 1: bbox=[450, 320, 480, 380], conf=0.92, class='blue_cone'
Detection 2: bbox=[720, 310, 750, 370], conf=0.85, class='yellow_cone'
```

**After (World Coordinates):**
```
Cone 1: x=8.2m ahead, y=-1.5m left, color=b, conf=0.92
Cone 2: x=8.5m ahead, y=1.2m right, color=y, conf=0.85
```

---

## üéØ **Success Criteria**

You're ready to move to SLAM when:

- [x] Camera parameters extracted from EUFS sim
- [x] Distance estimation implemented
- [x] Lateral offset calculation working
- [x] Tested on 10+ images
- [x] Accuracy validated (<0.5m error)
- [x] Output in standardized format

---

## üí° **Pro Tips**

1. **Start Simple**
   - Ignore camera tilt initially
   - Assume flat ground
   - Refine later

2. **Use Visualization**
   - Draw world coordinates on image
   - Helps debug issues quickly
   - Shows if estimates make sense

3. **Test Edge Cases**
   - Very close cones (<3m)
   - Far cones (>15m)
   - Cones at image edges
   - Partially occluded cones

4. **Document Assumptions**
   - Camera height
   - Cone height
   - Ground plane assumption
   - Important for debugging!

---

## üöÄ **After This Phase**

Once coordinate transformation works:

1. **Week 4-5:** Implement SLAM (map building)
2. **Week 6-7:** Path planning (racing line)
3. **Week 8-9:** Pure pursuit control
4. **Week 10:** Integration & testing

You're on track for July 2026! üèÅ

---

## üìû **Need Help?**

**Stuck on camera params?**
- Check EUFS sim documentation
- Look for `.yaml` config files
- Search for "camera_info" topics in ROS

**Math not working?**
- Draw it out on paper
- Use known distances to calibrate
- Start with rough estimates, refine later

**Validation failing?**
- Check coordinate frame conventions
- Verify camera mounting assumptions
- Test with simple straight-line case first

---

**You've got this! The hard part (perception) is DONE. This is just geometry! üéØ**
